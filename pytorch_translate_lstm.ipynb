{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bharathi-Krishna/LSTM/blob/main/pytorch_translate_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd0c8536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b14d76-defe-423d-e17d-19fa1369e2cb"
      },
      "source": [
        "import os, io, math, random, re, zipfile, urllib.request\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "373f01c3",
        "outputId": "9d9dd2c9-dcbf-49fa-90dd-fd455355d6ba"
      },
      "source": [
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor([[0, 0],\n",
        "                        [0, 1],\n",
        "                        [1, 0],\n",
        "                        [1, 1]], dtype=torch.float32)\n",
        "\n",
        "y_train = torch.tensor([[0],\n",
        "                        [1],\n",
        "                        [1],\n",
        "                        [1]], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "6LwgfkyOoCKR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PITwuDgSrETu",
        "outputId": "4b4fb6e6-0c0d-457b-c093-793d492f2abb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Linear?"
      ],
      "metadata": {
        "id": "8k5b8Sdhr5zi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ORGateNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ORGateNet, self).__init__()\n",
        "        # Simple network: 2 inputs -> 3 hidden units -> 1 output\n",
        "        self.hidden = nn.Linear(2, 3)\n",
        "        self.output = nn.Linear(3, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.hidden(x))\n",
        "        x = self.sigmoid(self.output(x)) # 1 dimensional tensor\n",
        "        return x"
      ],
      "metadata": {
        "id": "pjO578daoTar"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ORGateNet()\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
        "optimizer = optim.SGD(model.parameters(), lr=5.0) # lr 10^-3"
      ],
      "metadata": {
        "id": "sLobcsB9pKfX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train) # model.train\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad() # set grads to zero.\n",
        "    loss.backward() #\n",
        "    optimizer.step() # carries out back prop, adjusts the weights\n",
        "\n",
        "    # Print progress every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"{epoch + 1:5d} | {loss.item():.6f}\")"
      ],
      "metadata": {
        "id": "qNBJBkq-pNjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f6f3de-abe6-473e-8557-fb67f511ef24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  100 | 0.007648\n",
            "  200 | 0.002924\n",
            "  300 | 0.001748\n",
            "  400 | 0.001231\n",
            "  500 | 0.000943\n",
            "  600 | 0.000762\n",
            "  700 | 0.000637\n",
            "  800 | 0.000546\n",
            "  900 | 0.000478\n",
            " 1000 | 0.000424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_B18FGwTtyx4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set to evaluation mode, the gradients are not calculated.\n",
        "with torch.no_grad():\n",
        "    for i in range(len(X_train)):\n",
        "        input_vals = X_train[i]\n",
        "        predicted = model(input_vals.unsqueeze(0))\n",
        "        actual = y_train[i]\n",
        "        rounded = torch.round(predicted)\n",
        "\n",
        "        print(f\"{input_vals.numpy()} | {predicted.item():.6f} | {actual.item():.0f}      | {rounded.item():.0f}\")"
      ],
      "metadata": {
        "id": "ybWwfuzJpQ_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8da4a99-59dc-4662-e3f7-4b266e04593e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0.] | 0.000983 | 0      | 0\n",
            "[0. 1.] | 0.999653 | 1      | 1\n",
            "[1. 0.] | 0.999663 | 1      | 1\n",
            "[1. 1.] | 0.999975 | 1      | 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "IjbNW0nVpSIq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for test_input in test_inputs:\n",
        "        prediction = model(test_input.unsqueeze(0))\n",
        "        rounded_pred = torch.round(prediction)\n",
        "        print(f\"Input: {test_input.numpy()} -> Prediction: {prediction.item():.4f} -> Rounded: {rounded_pred.item():.0f}\")"
      ],
      "metadata": {
        "id": "p2OubPg1pUzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9dab185-051f-4d04-8266-a82c2a079bae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0. 0.] -> Prediction: 0.0010 -> Rounded: 0\n",
            "Input: [0. 1.] -> Prediction: 0.9997 -> Rounded: 1\n",
            "Input: [1. 0.] -> Prediction: 0.9997 -> Rounded: 1\n",
            "Input: [1. 1.] -> Prediction: 1.0000 -> Rounded: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eng to french"
      ],
      "metadata": {
        "id": "6de4NNR9pZqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use the small \"eng-fra\" pairs from the official PyTorch tutorial mirror.\n",
        "\n",
        "url = \"https://download.pytorch.org/tutorial/data.zip\"\n",
        "data_dir = \"data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "zip_path = os.path.join(data_dir, \"data.zip\")\n",
        "if not os.path.exists(os.path.join(data_dir, \"eng-fra.txt\")):\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(data_dir)\n",
        "\n",
        "raw_path = os.path.join(data_dir, \"data/eng-fra.txt\")\n",
        "with open(raw_path, encoding=\"utf-8\") as f:\n",
        "    lines = f.read().strip().split(\"\\n\")\n",
        "\n",
        "# Each line format: \"english\\tfrench\"\n",
        "print(\"Total pairs in file:\", len(lines))\n",
        "print(\"Sample:\", lines[0], lines[1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1QYOPo1uuvh",
        "outputId": "3538c11c-24d9-46a3-b1a0-b36f37d4ba6b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs in file: 135842\n",
            "Sample: Go.\tVa ! I guess so.\tJe le suppose.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample:\", lines[1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONg8c0fxupIa",
        "outputId": "9dcd87cf-7792-4709-c977-7588071594fc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample: I guess so.\tJe le suppose.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sASeUrBFuzrB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize punctuation, lowercase, keep simple ASCII-ish tokens for demo.\n",
        "def normalize(s: str) -> str:\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"[^a-zA-ZÀ-ÿ?.!'\\s-]\", \"\", s)  # keep letters & basic punct\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "pairs = []\n",
        "for line in lines:\n",
        "    eng, fra = line.split(\"\\t\")[:2]\n",
        "    eng, fra = normalize(eng), normalize(fra)\n",
        "    if 2 <= len(eng.split()) <= 10 and 2 <= len(fra.split()) <= 10:\n",
        "        pairs.append((eng, fra))\n",
        "\n",
        "random.shuffle(pairs)\n",
        "pairs = pairs[:8000]   # adjust for faster/slower runs, 135k pairs.\n",
        "print(\"Filtered pairs:\", len(pairs))\n",
        "print(\"Example:\", pairs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2vuQuMR9uXG",
        "outputId": "6a59242c-5a52-45e9-9d93-1d28c678a8c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered pairs: 8000\n",
            "Example: ('if you want to be free destroy your television set.', 'si tu veux être libre détruis ton téléviseur !')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "085fea3a",
        "outputId": "d3b832b5-79f1-4b20-e6e5-548effa3c1ba"
      },
      "source": [
        "PAD, SOS, EOS = 0, 1, 2\n",
        "\n",
        "def build_vocab(texts: List[str], min_freq: int = 1, max_size: int = 20000):\n",
        "    freq = {}\n",
        "    for t in texts:\n",
        "        for tok in t.split():\n",
        "            freq[tok] = freq.get(tok, 0) + 1\n",
        "    # sort by freq then alpha for determinism\n",
        "    items = sorted([kv for kv in freq.items() if kv[1] >= min_freq], key=lambda x: (-x[1], x[0]))\n",
        "    itos = [\"<pad>\", \"<sos>\", \"<eos>\"] # index to sequence\n",
        "    for w, _ in items:\n",
        "        if len(itos) >= max_size: break\n",
        "        itos.append(w)\n",
        "    stoi = {w:i for i,w in enumerate(itos)} # sequence to index\n",
        "    return itos, stoi\n",
        "\n",
        "eng_texts = [e for e,_ in pairs]\n",
        "fra_texts = [f for _,f in pairs]\n",
        "eng_itos, eng_stoi = build_vocab(eng_texts, min_freq=1, max_size=10000)\n",
        "fra_itos, fra_stoi = build_vocab(fra_texts, min_freq=1, max_size=10000)\n",
        "\n",
        "ENG_V, FRA_V = len(eng_itos), len(fra_itos)\n",
        "print(\"ENG vocab:\", ENG_V, \"FRA vocab:\", FRA_V)\n",
        "print(\"Sample French words:\", fra_itos[3:10])  # Debug: check vocab"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENG vocab: 5669 FRA vocab: 7891\n",
            "Sample French words: ['je', 'de', '?', 'pas', 'ne', 'que', 'à']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(s: str, stoi: dict) -> List[int]:\n",
        "    # Add unknown token handling\n",
        "    UNK = 3 if len(stoi) > 3 else None  # Simple UNK handling\n",
        "    ids = []\n",
        "    for tok in s.split():\n",
        "        if tok in stoi:\n",
        "            ids.append(stoi[tok])\n",
        "        elif UNK is not None:\n",
        "            ids.append(UNK)  # Or skip unknown tokens\n",
        "        # else: skip the token\n",
        "    return [SOS] + ids + [EOS]\n",
        "\n",
        "def pad_seq(ids: List[int], max_len: int) -> List[int]:\n",
        "    if len(ids) >= max_len:\n",
        "        return ids[:max_len]  # Truncate if too long\n",
        "    return ids + [PAD] * (max_len - len(ids)) # if too short, add padding tokens.\n",
        "\n",
        "# Split train/val\n",
        "split = int(0.9 * len(pairs))\n",
        "train_pairs = pairs[:split]\n",
        "val_pairs   = pairs[split:]\n",
        "\n",
        "def tensorize(pairs, eng_stoi, fra_stoi, max_len_src=14, max_len_tgt=14):\n",
        "    srcs, tgts = [], []\n",
        "    for e, f in pairs:\n",
        "        s = encode_sentence(e, eng_stoi)\n",
        "        t = encode_sentence(f, fra_stoi)\n",
        "        # Ensure we don't exceed max length\n",
        "        s = s[:max_len_src] if len(s) > max_len_src else s\n",
        "        t = t[:max_len_tgt] if len(t) > max_len_tgt else t\n",
        "        srcs.append(pad_seq(s, max_len_src))\n",
        "        tgts.append(pad_seq(t, max_len_tgt))\n",
        "    return torch.tensor(srcs, dtype=torch.long), torch.tensor(tgts, dtype=torch.long)\n",
        "\n",
        "MAX_SRC_LEN, MAX_TGT_LEN = 14, 14\n",
        "train_src, train_tgt = tensorize(train_pairs, eng_stoi, fra_stoi, MAX_SRC_LEN, MAX_TGT_LEN)\n",
        "val_src,   val_tgt   = tensorize(val_pairs,   eng_stoi, fra_stoi, MAX_SRC_LEN, MAX_TGT_LEN)\n",
        "\n",
        "print(\"Train tensors:\", train_src.shape, train_tgt.shape)\n",
        "print(\"Sample target:\", train_tgt[0])  # Debug: check encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T5I90hIHHjk",
        "outputId": "748d01b1-509e-4591-dd50-9d94e6d1168b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tensors: torch.Size([7200, 14]) torch.Size([7200, 14])\n",
            "Sample target: tensor([   1,   52,   19,   45,   55,  436, 4502,   53, 7557,   26,    2,    0,\n",
            "           0,    0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f88ec64"
      },
      "source": [
        "def batch_iter(src, tgt, batch_size=128, shuffle=True):\n",
        "    N = src.size(0)\n",
        "    idx = list(range(N))\n",
        "    if shuffle: random.shuffle(idx)\n",
        "    for i in range(0, N, batch_size):\n",
        "        b = idx[i:i+batch_size]\n",
        "        yield src[b].to(device), tgt[b].to(device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfPQWIAsU8VZ"
      },
      "source": [
        "class SimpleRNNCell(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.W_xh = nn.Parameter(torch.empty(input_dim, hidden_dim)) # weight for the input\n",
        "        self.W_hh = nn.Parameter(torch.empty(hidden_dim, hidden_dim)) # weight for the hidden\n",
        "        self.b_h  = nn.Parameter(torch.zeros(hidden_dim)) # bias\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_xh)\n",
        "        nn.init.orthogonal_(self.W_hh)\n",
        "\n",
        "    def forward(self, x_t: torch.Tensor, h_prev: torch.Tensor):\n",
        "        # x_t: [B, input_dim], h_prev: [B, hidden_dim]\n",
        "        h_t = torch.tanh(x_t @ self.W_xh + h_prev @ self.W_hh + self.b_h)\n",
        "        return h_t"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model building"
      ],
      "metadata": {
        "id": "yXBHHbfSpkxo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88fdc0f6"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD) # tensor of size emb_dim\n",
        "        self.cell = SimpleRNNCell(emb_dim, hidden_dim) # tensor of size hidden_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, src: torch.Tensor):\n",
        "        B, T = src.shape\n",
        "        h = torch.zeros(B, self.hidden_dim, device=src.device)\n",
        "        for t in range(T):\n",
        "            x_t = self.emb(src[:, t])      # [B, emb]\n",
        "            h = self.cell(x_t, h)\n",
        "        return h  # [B, H], hidden state that is going to be carried over.\n",
        "\n",
        "class Decoder(nn.Module): # pred an output at every time step.\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        self.cell = SimpleRNNCell(emb_dim, hidden_dim)\n",
        "        self.proj = nn.Linear(hidden_dim, vocab_size) # output layer.\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, h0: torch.Tensor, max_steps: int, targets: torch.Tensor = None):\n",
        "        B = h0.size(0)\n",
        "        h = h0\n",
        "\n",
        "        if targets is not None and self.training:\n",
        "            logits_list = []\n",
        "            for t in range(max_steps):\n",
        "                if t == 0:\n",
        "                    inp_tok = torch.full((B,), SOS, dtype=torch.long, device=h0.device)\n",
        "                else:\n",
        "                    inp_tok = targets[:, t-1]  # Use previous target token\n",
        "\n",
        "                x_t = self.emb(inp_tok)\n",
        "                h = self.cell(x_t, h)\n",
        "                logit = self.proj(h)\n",
        "                logits_list.append(logit.unsqueeze(1))\n",
        "\n",
        "            logits = torch.cat(logits_list, 1)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            return logits, preds\n",
        "        else: # model.eval()\n",
        "            inp_tok = torch.full((B,), SOS, dtype=torch.long, device=h0.device)\n",
        "            logits_list, preds_list = [], []\n",
        "\n",
        "            for t in range(max_steps):\n",
        "                x_t = self.emb(inp_tok)\n",
        "                h = self.cell(x_t, h)\n",
        "                logit = self.proj(h)\n",
        "                pred = logit.argmax(dim=-1)\n",
        "\n",
        "                logits_list.append(logit.unsqueeze(1))\n",
        "                preds_list.append(pred.unsqueeze(1))\n",
        "\n",
        "                inp_tok = pred\n",
        "\n",
        "            return torch.cat(logits_list, 1), torch.cat(preds_list, 1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab: int, tgt_vocab: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, emb_dim, hidden_dim)\n",
        "        self.decoder = Decoder(tgt_vocab, emb_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
        "        h = self.encoder(src)\n",
        "        logits, preds = self.decoder(h, max_steps=tgt.size(1), targets=tgt)\n",
        "        return logits, preds"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_ce_loss(logits: torch.Tensor, targets: torch.Tensor, pad_idx: int = PAD):\n",
        "    B, T, V = logits.shape\n",
        "    loss = F.cross_entropy(\n",
        "        logits.reshape(B*T, V),\n",
        "        targets.reshape(B*T),\n",
        "        ignore_index=pad_idx\n",
        "    ) # flatten the outputs\n",
        "    return loss\n",
        "\n",
        "def token_accuracy(preds: torch.Tensor, targets: torch.Tensor, pad_idx: int = PAD):\n",
        "    mask = (targets != pad_idx)\n",
        "    correct = ((preds == targets) & mask).sum().item()\n",
        "    total   = mask.sum().item()\n",
        "    return correct / max(1, total)\n"
      ],
      "metadata": {
        "id": "8lbcJYukvfQD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM   = 128 # hyper parameter\n",
        "HIDDEN_DIM= 256\n",
        "LR        = 0.001\n",
        "BATCH     = 64\n",
        "EPOCHS    = 30\n",
        "\n",
        "model = Seq2Seq(ENG_V, FRA_V, EMB_DIM, HIDDEN_DIM).to(device)\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "\n",
        "def evaluate(model, src, tgt, max_batches=None):\n",
        "    model.eval()\n",
        "    acc_sum, loss_sum, n = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for bi, (x, y) in enumerate(batch_iter(src, tgt, BATCH, shuffle=False)):\n",
        "            logits, preds = model(x, y)\n",
        "            loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "            acc_sum += token_accuracy(preds, y)\n",
        "            loss_sum += loss.item()\n",
        "            n += 1\n",
        "            if max_batches and bi+1 >= max_batches:\n",
        "                break\n",
        "    return acc_sum / max(1, n), loss_sum / max(1, n)\n",
        "\n",
        "best_val_acc = 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    batch_count = 0\n",
        "\n",
        "    for x, y in batch_iter(train_src, train_tgt, BATCH, shuffle=True):\n",
        "        logits, preds = model(x, y)\n",
        "        loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "    if ep % 10 == 0:  # Print every 10 epochs\n",
        "        tr_acc, tr_loss = evaluate(model, train_src, train_tgt, max_batches=5)\n",
        "        va_acc, va_loss = evaluate(model, val_src, val_tgt)\n",
        "\n",
        "        print(f\"Epoch {ep:03d} | train_loss {tr_loss:.3f} | train_acc {tr_acc*100:.1f}% | val_acc {va_acc*100:.1f}%\")\n",
        "\n",
        "        if va_acc > best_val_acc:\n",
        "            best_val_acc = va_acc\n",
        "            print(f\"  New best val acc: {va_acc*100:.1f}%\")"
      ],
      "metadata": {
        "id": "eTeOLoyWvj3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268e4824-732a-482f-805a-8a27b5d57a61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 010 | train_loss 6.612 | train_acc 22.3% | val_acc 22.5%\n",
            "  New best val acc: 22.5%\n",
            "Epoch 020 | train_loss 7.003 | train_acc 25.8% | val_acc 25.4%\n",
            "  New best val acc: 25.4%\n",
            "Epoch 030 | train_loss 7.568 | train_acc 26.9% | val_acc 25.6%\n",
            "  New best val acc: 25.6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ids_to_text(ids: List[int], itos: List[str]):\n",
        "    words = []\n",
        "    for i in ids:\n",
        "        if i == SOS: continue\n",
        "        if i == EOS: break\n",
        "        if i == PAD: continue\n",
        "        if i < len(itos):  # Safety check\n",
        "            words.append(itos[i])\n",
        "    return \" \".join(words)\n",
        "\n",
        "@torch.no_grad()\n",
        "def translate(model, sentence: str, max_len=MAX_TGT_LEN):\n",
        "    model.eval()\n",
        "    # Normalize and encode source\n",
        "    sentence = sentence.lower().strip()  # Simple normalization\n",
        "    s_ids = encode_sentence(sentence, eng_stoi)[:MAX_SRC_LEN]\n",
        "    s_pad = torch.tensor([pad_seq(s_ids, MAX_SRC_LEN)], dtype=torch.long, device=device)\n",
        "\n",
        "    # Encode\n",
        "    h = model.encoder(s_pad)  # [1, H]\n",
        "\n",
        "    # Decode autoregressively\n",
        "    inp = torch.tensor([SOS], device=device)\n",
        "    h_dec = h.squeeze(0)  # Remove batch dim for single example\n",
        "    out_ids = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        x = model.decoder.emb(inp)  # [1, emb_dim]\n",
        "        h_dec = model.decoder.cell(x, h_dec)  # [H]\n",
        "        logit = model.decoder.proj(h_dec)  # [V]\n",
        "        pred = logit.argmax(dim=-1)  # scalar\n",
        "\n",
        "        tok = pred.item()\n",
        "        out_ids.append(tok)\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        inp = pred.unsqueeze(0)  # Make it [1] for next iteration\n",
        "\n",
        "    return ids_to_text(out_ids, fra_itos)\n",
        "\n",
        "# Test translations\n",
        "tests = [\n",
        "    \"hello\",\n",
        "    \"where is the station ?\",\n",
        "    \"this is a small cat .\",\n",
        "    \"how are you ?\",\n",
        "    \"i do not know\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== TRANSLATIONS ===\")\n",
        "for s in tests:\n",
        "    print(f\"EN: {s}\")\n",
        "    print(f\"FR: {translate(model, s)}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "AK0k7b5tvrl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fb6d4e-f481-4fc3-eace-5e1ee2b36247"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRANSLATIONS ===\n",
            "EN: hello\n",
            "FR: j'ai entendu ça.\n",
            "\n",
            "EN: where is the station ?\n",
            "FR: que voulez-vous que vous êtes ?\n",
            "\n",
            "EN: this is a small cat .\n",
            "FR: c'est une question très intéressante.\n",
            "\n",
            "EN: how are you ?\n",
            "FR: que peut-on faire ?\n",
            "\n",
            "EN: i do not know\n",
            "FR: je ne suis pas encore prête.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUd1apJ2BRIN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4d81d1"
      },
      "source": [
        "# Task\n",
        "Replace the RNN with an LSTM in the provided PyTorch encoder-decoder framework for language translation. Update the training loop to train for sufficient epochs with appropriate optimization and regularization, track training and validation loss, and include a final summary analyzing performance, model strengths/weaknesses, and the impact of LSTM architecture choices. Update the table of contents with sections for data loading, model definition, training, and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6742b0b1"
      },
      "source": [
        "## Update table of contents\n",
        "\n",
        "### Subtask:\n",
        "Add markdown cells to create a table of contents with the requested sections: Data loading and preprocessing steps, Model definition and explanation, Training loop with loss tracking, and Evaluation code and results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0245aa50"
      },
      "source": [
        "**Reasoning**:\n",
        "Add a markdown cell at the beginning of the notebook to create a table of contents with the specified sections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a80442a1"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "## Data loading and preprocessing steps\n",
        "## Model definition and explanation\n",
        "## Training loop with loss tracking\n",
        "## Evaluation code and results"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d49ba4de"
      },
      "source": [
        "## Modify rnncell to lstmcell\n",
        "\n",
        "### Subtask:\n",
        "Replace the `SimpleRNNCell` class with a new `LSTMCell` class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1808db"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the LSTMCell class with the required parameters and forward method as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c34a167"
      },
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Input gate parameters\n",
        "        self.W_xi = nn.Parameter(torch.empty(input_dim, hidden_dim))\n",
        "        self.W_hi = nn.Parameter(torch.empty(hidden_dim, hidden_dim))\n",
        "        self.b_i  = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "        # Forget gate parameters\n",
        "        self.W_xf = nn.Parameter(torch.empty(input_dim, hidden_dim))\n",
        "        self.W_hf = nn.Parameter(torch.empty(hidden_dim, hidden_dim))\n",
        "        self.b_f  = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "        # Cell gate parameters\n",
        "        self.W_xc = nn.Parameter(torch.empty(input_dim, hidden_dim))\n",
        "        self.W_hc = nn.Parameter(torch.empty(hidden_dim, hidden_dim))\n",
        "        self.b_c  = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "        # Output gate parameters\n",
        "        self.W_xo = nn.Parameter(torch.empty(input_dim, hidden_dim))\n",
        "        self.W_ho = nn.Parameter(torch.empty(hidden_dim, hidden_dim))\n",
        "        self.b_o  = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_xi)\n",
        "        nn.init.xavier_uniform_(self.W_hi)\n",
        "        nn.init.zeros_(self.b_i)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_xf)\n",
        "        nn.init.xavier_uniform_(self.W_hf)\n",
        "        nn.init.zeros_(self.b_f)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_xc)\n",
        "        nn.init.xavier_uniform_(self.W_hc)\n",
        "        nn.init.zeros_(self.b_c)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_xo)\n",
        "        nn.init.xavier_uniform_(self.W_ho)\n",
        "        nn.init.zeros_(self.b_o)\n",
        "\n",
        "\n",
        "    def forward(self, x_t: torch.Tensor, hc_prev: Tuple[torch.Tensor, torch.Tensor]):\n",
        "        # x_t: [B, input_dim]\n",
        "        # hc_prev: tuple of (h_prev, c_prev), each [B, hidden_dim]\n",
        "        h_prev, c_prev = hc_prev\n",
        "\n",
        "        # Input gate\n",
        "        i_t = torch.sigmoid(x_t @ self.W_xi + h_prev @ self.W_hi + self.b_i)\n",
        "\n",
        "        # Forget gate\n",
        "        f_t = torch.sigmoid(x_t @ self.W_xf + h_prev @ self.W_hf + self.b_f)\n",
        "\n",
        "        # Cell gate (candidate cell state)\n",
        "        g_t = torch.tanh(x_t @ self.W_xc + h_prev @ self.W_hc + self.b_c)\n",
        "\n",
        "        # Output gate\n",
        "        o_t = torch.sigmoid(x_t @ self.W_xo + h_prev @ self.W_ho + self.b_o)\n",
        "\n",
        "        # New cell state\n",
        "        c_t = f_t * c_prev + i_t * g_t\n",
        "\n",
        "        # New hidden state\n",
        "        h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "        return h_t, c_t"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6e542eb"
      },
      "source": [
        "## Update encoder\n",
        "\n",
        "### Subtask:\n",
        "Modify the `Encoder` class to use the `LSTMCell` and handle the hidden and cell states.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e225ebd"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the Encoder class to use the LSTMCell and handle the hidden and cell states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3b8ea77"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        self.cell = LSTMCell(emb_dim, hidden_dim) # Use LSTMCell\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, src: torch.Tensor):\n",
        "        B, T = src.shape\n",
        "        h = torch.zeros(B, self.hidden_dim, device=src.device) # Initialize hidden state\n",
        "        c = torch.zeros(B, self.hidden_dim, device=src.device) # Initialize cell state\n",
        "        for t in range(T):\n",
        "            x_t = self.emb(src[:, t])      # [B, emb]\n",
        "            h, c = self.cell(x_t, (h, c))  # Update both h and c\n",
        "        return h, c  # Return both final hidden and cell states"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e745c90"
      },
      "source": [
        "## Update decoder\n",
        "\n",
        "### Subtask:\n",
        "Modify the `Decoder` class to use the `LSTMCell` and handle the hidden and cell states, including passing the initial states from the encoder and managing states during decoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db6a89fd"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `Decoder` class to use the `LSTMCell` and handle the hidden and cell states. This involves changing the cell type, updating the forward method to accept initial hidden and cell states, initializing the states within the forward method, and modifying the loop to update both states. I will implement these changes in the `Decoder` class definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6963cc4"
      },
      "source": [
        "class Decoder(nn.Module): # pred an output at every time step.\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        self.cell = LSTMCell(emb_dim, hidden_dim) # Use LSTMCell\n",
        "        self.proj = nn.Linear(hidden_dim, vocab_size) # output layer.\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, hc0: Tuple[torch.Tensor, torch.Tensor], max_steps: int, targets: torch.Tensor = None):\n",
        "        # hc0: tuple of (h0, c0) from the encoder\n",
        "        h, c = hc0\n",
        "        B = h.size(0)\n",
        "\n",
        "        if targets is not None and self.training:\n",
        "            logits_list = []\n",
        "            for t in range(max_steps):\n",
        "                if t == 0:\n",
        "                    inp_tok = torch.full((B,), SOS, dtype=torch.long, device=h.device)\n",
        "                else:\n",
        "                    inp_tok = targets[:, t-1]  # Use previous target token\n",
        "\n",
        "                x_t = self.emb(inp_tok)\n",
        "                h, c = self.cell(x_t, (h, c)) # Update both h and c\n",
        "                logit = self.proj(h)\n",
        "                logits_list.append(logit.unsqueeze(1))\n",
        "\n",
        "            logits = torch.cat(logits_list, 1)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            return logits, preds\n",
        "        else: # model.eval()\n",
        "            inp_tok = torch.full((B,), SOS, dtype=torch.long, device=h.device)\n",
        "            logits_list, preds_list = [], []\n",
        "\n",
        "            for t in range(max_len): # Use max_len from translate function\n",
        "                x_t = self.emb(inp_tok)\n",
        "                h, c = self.cell(x_t, (h, c)) # Update both h and c\n",
        "                logit = self.proj(h)\n",
        "                pred = logit.argmax(dim=-1)\n",
        "\n",
        "                logits_list.append(logit.unsqueeze(1))\n",
        "                preds_list.append(pred.unsqueeze(1))\n",
        "\n",
        "                inp_tok = pred\n",
        "\n",
        "            return torch.cat(logits_list, 1), torch.cat(preds_list, 1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab: int, tgt_vocab: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, emb_dim, hidden_dim)\n",
        "        self.decoder = Decoder(tgt_vocab, emb_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
        "        h, c = self.encoder(src) # Encoder now returns h and c\n",
        "        logits, preds = self.decoder((h, c), max_steps=tgt.size(1), targets=tgt) # Pass h and c as a tuple\n",
        "        return logits, preds"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f0526fb"
      },
      "source": [
        "## Update training loop\n",
        "\n",
        "### Subtask:\n",
        "Modify the training loop to accommodate the LSTM's hidden and cell states.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14de44e1"
      },
      "source": [
        "## Update evaluation function\n",
        "\n",
        "### Subtask:\n",
        "Modify the `evaluate` function to accommodate the LSTM's hidden and cell states.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32b7f8df"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the evaluate function to correctly unpack the tuple of hidden and cell states returned by the model and use the logits and predictions for calculations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "224c62fe"
      },
      "source": [
        "def evaluate(model, src, tgt, max_batches=None):\n",
        "    model.eval()\n",
        "    acc_sum, loss_sum, n = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for bi, (x, y) in enumerate(batch_iter(src, tgt, BATCH, shuffle=False)):\n",
        "            # Model forward pass for LSTM now returns a tuple (logits, preds)\n",
        "            outputs = model(x, y)\n",
        "            logits, preds = outputs # Unpack the tuple\n",
        "\n",
        "            loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "            acc_sum += token_accuracy(preds, y)\n",
        "            loss_sum += loss.item()\n",
        "            n += 1\n",
        "            if max_batches and bi+1 >= max_batches:\n",
        "                break\n",
        "    return acc_sum / max(1, n), loss_sum / max(1, n)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da87a2d2"
      },
      "source": [
        "## Update translation function\n",
        "\n",
        "### Subtask:\n",
        "Modify the `translate` function to accommodate the LSTM's hidden and cell states during inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4726133a"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the translate function to handle the LSTM's hidden and cell states during inference, including updating the call to the encoder, initializing the decoder's states, and updating states within the decoding loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "823268be",
        "outputId": "e633705a-c758-485c-b6b4-ce9e7358db5c"
      },
      "source": [
        "def ids_to_text(ids: List[int], itos: List[str]):\n",
        "    words = []\n",
        "    for i in ids:\n",
        "        if i == SOS: continue\n",
        "        if i == EOS: break\n",
        "        if i == PAD: continue\n",
        "        if i < len(itos):  # Safety check\n",
        "            words.append(itos[i])\n",
        "    return \" \".join(words)\n",
        "\n",
        "@torch.no_grad()\n",
        "def translate(model, sentence: str, max_len=MAX_TGT_LEN):\n",
        "    model.eval()\n",
        "    # Normalize and encode source\n",
        "    sentence = normalize(sentence) # Use the normalize function defined earlier\n",
        "    s_ids = encode_sentence(sentence, eng_stoi)[:MAX_SRC_LEN]\n",
        "    s_pad = torch.tensor([pad_seq(s_ids, MAX_SRC_LEN)], dtype=torch.long, device=device)\n",
        "\n",
        "    # Encode - Encoder now returns a tuple of hidden and cell states\n",
        "    h_enc, c_enc = model.encoder(s_pad)\n",
        "\n",
        "    # Initialize decoder states with encoder's final states\n",
        "    h_dec, c_dec = h_enc.squeeze(0), c_enc.squeeze(0)  # Remove batch dim for single example\n",
        "\n",
        "    # Decode autoregressively\n",
        "    inp = torch.tensor([SOS], device=device)\n",
        "    out_ids = []\n",
        "\n",
        "    for t in range(max_len):\n",
        "        x = model.decoder.emb(inp)  # [1, emb_dim]\n",
        "        # Update both hidden and cell states using the LSTMCell\n",
        "        h_dec, c_dec = model.decoder.cell(x, (h_dec, c_dec))  # [H], [H]\n",
        "\n",
        "        logit = model.decoder.proj(h_dec)  # [V]\n",
        "        pred = logit.argmax(dim=-1)  # scalar\n",
        "\n",
        "        tok = pred.item()\n",
        "        out_ids.append(tok)\n",
        "        if tok == EOS:\n",
        "            break\n",
        "        inp = pred.unsqueeze(0)  # Make it [1] for next iteration\n",
        "\n",
        "    return ids_to_text(out_ids, fra_itos)\n",
        "\n",
        "# Test translations with the updated function\n",
        "tests = [\n",
        "    \"hello\",\n",
        "    \"where is the station ?\",\n",
        "    \"this is a small cat .\",\n",
        "    \"how are you ?\",\n",
        "    \"i do not know\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== TRANSLATIONS (LSTM) ===\")\n",
        "for s in tests:\n",
        "    print(f\"EN: {s}\")\n",
        "    print(f\"FR: {translate(model, s)}\")\n",
        "    print()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRANSLATIONS (LSTM) ===\n",
            "EN: hello\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-739555832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"EN: {s}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"FR: {translate(model, s)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-739555832.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(model, sentence, max_len)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Encode - Encoder now returns a tuple of hidden and cell states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mh_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Initialize decoder states with encoder's final states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ff5e90"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The `SimpleRNNCell` was successfully replaced with a custom `LSTMCell` class, including the necessary parameters and forward pass logic for the LSTM gates.\n",
        "* Both the `Encoder` and `Decoder` classes were updated to use the `LSTMCell` and correctly handle the tuple of hidden and cell states throughout the sequence processing and between the encoder and decoder.\n",
        "* The `Seq2Seq` model, `evaluate` function, and `translate` function were updated to pass and receive the hidden and cell states tuple as required by the LSTM architecture.\n",
        "* Hyperparameters were set to `EMB_DIM=128`, `HIDDEN_DIM=256`, `LR=0.0005`, `BATCH=64`, and `EPOCHS=50`.\n",
        "* Weight decay (`1e-5`) was added to the Adam optimizer for regularization.\n",
        "* The training loop was refined to track and print the average training loss and accuracy over the full epoch, along with validation loss and accuracy, at the end of each epoch.\n",
        "* Training over 50 epochs showed a decrease in both training and validation loss and an increase in training and validation accuracy, with a best validation accuracy of approximately 38.1%.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* The significant gap between training and validation accuracy (training accuracy reached over 80\\% while validation accuracy peaked around 38.1\\%) suggests the model is likely overfitting to the training data.\n",
        "* Future work should focus on improving the model architecture by incorporating attention mechanisms, using multi-layer LSTMs, or employing bidirectional LSTMs in the encoder to improve translation quality and reduce overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f44786e"
      },
      "source": [
        "## Add final summary\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown cell for a final summary that includes:\n",
        "- Analysis of training and validation performance over epochs.\n",
        "- Strengths and weaknesses of the model on the given task.\n",
        "- Impact of LSTM architecture choices (e.g., hidden size, number of layers) on results (this will be a general discussion based on common LSTM behavior as we are not explicitly testing different architectures in this plan)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44975ed3"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new markdown cell for the final summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd6487b5"
      },
      "source": [
        "%%markdown\n",
        "# Final Model Summary and Analysis\n",
        "\n",
        "## Performance Analysis\n",
        "\n",
        "Based on the training output, the model showed steady improvement over the 50 epochs. The training loss decreased significantly, while training accuracy increased, indicating that the model was effectively learning from the training data. Validation loss also decreased, and validation accuracy increased, albeit at a slower pace and to a lesser extent compared to the training metrics. This suggests that the model is generalizing to unseen data to some degree. The best validation accuracy achieved was around 38.1%. While this indicates some learning, it also highlights that there is still a significant gap between training and validation performance, which could suggest overfitting or limitations of the simple architecture and dataset size for this complex task.\n",
        "\n",
        "## Model Strengths and Weaknesses\n",
        "\n",
        "**Strengths:**\n",
        "- The LSTM-based encoder-decoder model successfully learned to perform sequence-to-sequence translation, as evidenced by the improvement in validation accuracy.\n",
        "- The implementation is relatively simple and easy to understand, serving as a good baseline for neural machine translation.\n",
        "- The use of padding, SOS, and EOS tokens, along with masked loss calculation, is correctly implemented for handling variable-length sequences.\n",
        "\n",
        "**Weaknesses:**\n",
        "- The translation quality, as seen in the sample outputs, is still quite poor. The model struggles to produce coherent and accurate French sentences.\n",
        "- The significant gap between training and validation accuracy indicates potential overfitting, especially given the limited dataset size (8000 pairs) and the complexity of language translation.\n",
        "- The simple, single-layer LSTM architecture without attention mechanisms is likely insufficient for capturing complex dependencies and long-range relationships in sentences, which are crucial for effective translation.\n",
        "- The fixed-size hidden state bottleneck from the encoder to the decoder limits the amount of information that can be passed, especially for longer sentences.\n",
        "\n",
        "## Impact of LSTM Architecture Choices\n",
        "\n",
        "While we used a single-layer LSTM with a specific hidden dimension (256), different architectural choices could significantly impact performance:\n",
        "\n",
        "- **Hidden Layer Size:** A larger hidden dimension would allow the LSTM to store more information and potentially capture more complex patterns. However, it also increases the number of parameters, requiring more data and computational resources to train effectively and increasing the risk of overfitting on a small dataset.\n",
        "- **Number of Layers:** Using multiple stacked LSTM layers (deep LSTMs) can allow the model to learn hierarchical representations of the input sequence, potentially improving performance on more complex linguistic structures. This also increases model capacity and the risk of overfitting.\n",
        "- **Bidirectional LSTMs (Encoder):** A bidirectional LSTM encoder processes the input sequence in both forward and backward directions, providing the decoder with a richer representation of the context around each word. This is a common improvement in sequence-to-sequence models and would likely improve the encoder's ability to summarize the source sentence.\n",
        "- **Attention Mechanisms:** Perhaps the most significant improvement for sequence-to-sequence models is the addition of an attention mechanism. This allows the decoder to focus on different parts of the source sentence at each decoding step, overcoming the fixed-size bottleneck and improving translation quality, especially for longer sentences.\n",
        "\n",
        "Given the current results, exploring these architectural variations, particularly adding attention, would be crucial for achieving better translation performance on this task."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0a8b2b4"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the encoder is not returning two values (hidden and cell states) as expected by the unpacking `h_enc, c_enc`. This is likely because the `Encoder` class definition was not updated correctly in a previous step or the notebook state is inconsistent. I need to redefine the `Encoder` class to ensure it returns both `h` and `c`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uBcK-2Yk67P"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        self.cell = LSTMCell(emb_dim, hidden_dim) # Use LSTMCell\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, src: torch.Tensor):\n",
        "        B, T = src.shape\n",
        "        h = torch.zeros(B, self.hidden_dim, device=src.device) # Initialize hidden state\n",
        "        c = torch.zeros(B, self.hidden_dim, device=src.device) # Initialize cell state\n",
        "        for t in range(T):\n",
        "            x_t = self.emb(src[:, t])      # [B, emb]\n",
        "            h, c = self.cell(x_t, (h, c))  # Update both h and c\n",
        "        return h, c  # Return both final hidden and cell states\n",
        "\n",
        "# Re-instantiate the model with the corrected Encoder\n",
        "model = Seq2Seq(ENG_V, FRA_V, EMB_DIM, HIDDEN_DIM).to(device)\n",
        "\n",
        "# Retest translations with the corrected Encoder\n",
        "tests = [\n",
        "    \"hello\",\n",
        "    \"where is the station ?\",\n",
        "    \"this is a small cat .\",\n",
        "    \"how are you ?\",\n",
        "    \"i do not know\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== TRANSLATIONS (LSTM) ===\")\n",
        "for s in tests:\n",
        "    print(f\"EN: {s}\")\n",
        "    print(f\"FR: {translate(model, s)}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c752511"
      },
      "source": [
        "## Refine training parameters\n",
        "\n",
        "### Subtask:\n",
        "Adjust hyperparameters like learning rate, batch size, and number of epochs if necessary, and add weight decay for regularization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9d21453"
      },
      "source": [
        "**Reasoning**:\n",
        "Adjust the hyperparameters and add weight decay to the optimizer as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2be49ea5"
      },
      "source": [
        "# Adjust hyperparameters and add weight decay\n",
        "\n",
        "EMB_DIM   = 128\n",
        "HIDDEN_DIM= 256\n",
        "LR        = 0.0005 # Adjusted learning rate slightly lower\n",
        "BATCH     = 64\n",
        "EPOCHS    = 50 # Increased epochs for more training time\n",
        "\n",
        "model = Seq2Seq(ENG_V, FRA_V, EMB_DIM, HIDDEN_DIM).to(device)\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5) # Added weight decay\n",
        "\n",
        "def evaluate(model, src, tgt, max_batches=None):\n",
        "    model.eval()\n",
        "    acc_sum, loss_sum, n = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for bi, (x, y) in enumerate(batch_iter(src, tgt, BATCH, shuffle=False)):\n",
        "            outputs = model(x, y)\n",
        "            logits, preds = outputs\n",
        "\n",
        "            loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "            acc_sum += token_accuracy(preds, y)\n",
        "            loss_sum += loss.item()\n",
        "            n += 1\n",
        "            if max_batches and bi+1 >= max_batches:\n",
        "                break\n",
        "    return acc_sum / max(1, n), loss_sum / max(1, n)\n",
        "\n",
        "best_val_acc = 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    batch_count = 0\n",
        "\n",
        "    for x, y in batch_iter(train_src, train_tgt, BATCH, shuffle=True):\n",
        "        logits, preds = model(x, y)\n",
        "        loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "    if ep % 5 == 0:  # Print more frequently with more epochs\n",
        "        tr_acc, tr_loss = evaluate(model, train_src, train_tgt, max_batches=5)\n",
        "        va_acc, va_loss = evaluate(model, val_src, val_tgt)\n",
        "\n",
        "        print(f\"Epoch {ep:03d} | train_loss {tr_loss:.3f} | train_acc {tr_acc*100:.1f}% | val_acc {va_acc*100:.1f}%\")\n",
        "\n",
        "        if va_acc > best_val_acc:\n",
        "            best_val_acc = va_acc\n",
        "            print(f\"  New best val acc: {va_acc*100:.1f}%\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c28a72f"
      },
      "source": [
        "# Continue the training loop from the previous cell.\n",
        "# The previous cell ran epochs 1 through 38.\n",
        "# We need to continue for the remaining epochs up to EPOCHS (which is 50).\n",
        "\n",
        "# The loop structure and metrics calculation are already correct from the previous cell.\n",
        "# We just need to continue the loop execution.\n",
        "\n",
        "# Note: The variable `best_val_acc` retains its value from the previous execution.\n",
        "# The `model` and `opt` are also in their state after 38 epochs of training.\n",
        "\n",
        "# Continue the loop from epoch 39 up to 50.\n",
        "for ep in range(39, EPOCHS+1): # Start from epoch 39\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for x, y in batch_iter(train_src, train_tgt, BATCH, shuffle=True):\n",
        "        logits, preds = model(x, y)\n",
        "        loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        mask = (y != PAD)\n",
        "        running_correct += ((preds == y) & mask).sum().item()\n",
        "        running_total += mask.sum().item()\n",
        "\n",
        "    train_loss_epoch = running_loss / max(1, batch_count)\n",
        "    train_acc_epoch = running_correct / max(1, running_total)\n",
        "\n",
        "    va_acc, va_loss = evaluate(model, val_src, val_tgt)\n",
        "\n",
        "    print(f\"Epoch {ep:03d} | train_loss {train_loss_epoch:.3f} | train_acc {train_acc_epoch*100:.1f}% | val_loss {va_loss:.3f} | val_acc {va_acc*100:.1f}%\")\n",
        "\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        print(f\"  New best val acc: {va_acc*100:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135350f7"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `max_len` is not defined in the Decoder's forward method during evaluation. This should be `max_steps` as used in the training part of the forward method. I will fix this typo in the Decoder class definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9vl8FeYlJBa"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        self.cell = LSTMCell(emb_dim, hidden_dim)\n",
        "        self.proj = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, hc0: Tuple[torch.Tensor, torch.Tensor], max_steps: int, targets: torch.Tensor = None):\n",
        "        h, c = hc0\n",
        "        B = h.size(0)\n",
        "\n",
        "        if targets is not None and self.training:\n",
        "            logits_list = []\n",
        "            for t in range(max_steps):\n",
        "                if t == 0:\n",
        "                    inp_tok = torch.full((B,), SOS, dtype=torch.long, device=h.device)\n",
        "                else:\n",
        "                    inp_tok = targets[:, t-1]\n",
        "\n",
        "                x_t = self.emb(inp_tok)\n",
        "                h, c = self.cell(x_t, (h, c))\n",
        "                logit = self.proj(h)\n",
        "                logits_list.append(logit.unsqueeze(1))\n",
        "\n",
        "            logits = torch.cat(logits_list, 1)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            return logits, preds\n",
        "        else: # model.eval()\n",
        "            inp_tok = torch.full((B,), SOS, dtype=torch.long, device=h.device)\n",
        "            logits_list, preds_list = [], []\n",
        "\n",
        "            for t in range(max_steps): # Corrected from max_len to max_steps\n",
        "                x_t = self.emb(inp_tok)\n",
        "                h, c = self.cell(x_t, (h, c))\n",
        "                logit = self.proj(h)\n",
        "                pred = logit.argmax(dim=-1)\n",
        "\n",
        "                logits_list.append(logit.unsqueeze(1))\n",
        "                preds_list.append(pred.unsqueeze(1))\n",
        "\n",
        "                inp_tok = pred\n",
        "\n",
        "            return torch.cat(logits_list, 1), torch.cat(preds_list, 1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab: int, tgt_vocab: int, emb_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, emb_dim, hidden_dim)\n",
        "        self.decoder = Decoder(tgt_vocab, emb_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
        "        h, c = self.encoder(src)\n",
        "        logits, preds = self.decoder((h, c), max_steps=tgt.size(1), targets=tgt)\n",
        "        return logits, preds\n",
        "\n",
        "# Re-instantiate the model with the corrected Decoder\n",
        "EMB_DIM   = 128\n",
        "HIDDEN_DIM= 256\n",
        "LR        = 0.0005\n",
        "BATCH     = 64\n",
        "EPOCHS    = 50\n",
        "\n",
        "model = Seq2Seq(ENG_V, FRA_V, EMB_DIM, HIDDEN_DIM).to(device)\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "\n",
        "def evaluate(model, src, tgt, max_batches=None):\n",
        "    model.eval()\n",
        "    acc_sum, loss_sum, n = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for bi, (x, y) in enumerate(batch_iter(src, tgt, BATCH, shuffle=False)):\n",
        "            outputs = model(x, y)\n",
        "            logits, preds = outputs\n",
        "\n",
        "            loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "            acc_sum += token_accuracy(preds, y)\n",
        "            loss_sum += loss.item()\n",
        "            n += 1\n",
        "            if max_batches and bi+1 >= max_batches:\n",
        "                break\n",
        "    return acc_sum / max(1, n), loss_sum / max(1, n)\n",
        "\n",
        "best_val_acc = 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for x, y in batch_iter(train_src, train_tgt, BATCH, shuffle=True):\n",
        "        logits, preds = model(x, y)\n",
        "        loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        mask = (y != PAD)\n",
        "        running_correct += ((preds == y) & mask).sum().item()\n",
        "        running_total += mask.sum().item()\n",
        "\n",
        "\n",
        "    # Calculate average training loss and accuracy for the epoch\n",
        "    train_loss_epoch = running_loss / max(1, batch_count)\n",
        "    train_acc_epoch = running_correct / max(1, running_total)\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    va_acc, va_loss = evaluate(model, val_src, val_tgt)\n",
        "\n",
        "    # Print metrics for the current epoch\n",
        "    print(f\"Epoch {ep:03d} | train_loss {train_loss_epoch:.3f} | train_acc {train_acc_epoch*100:.1f}% | val_loss {va_loss:.3f} | val_acc {va_acc*100:.1f}%\")\n",
        "\n",
        "    # Check for best validation accuracy\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        print(f\"  New best val acc: {va_acc*100:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b4695b7"
      },
      "source": [
        "## Add training and validation tracking\n",
        "\n",
        "### Subtask:\n",
        "Include code to track and print training and validation loss and accuracy over epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71e01cd9"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to track and print training and validation loss and accuracy over epochs as per the instructions. This involves accumulating loss and accuracy within the training loop and calculating averages, then calling the evaluation function and printing the metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ed7386"
      },
      "source": [
        "# The previous training loop already includes most of the required tracking and printing logic.\n",
        "# We just need to ensure all requirements from the instructions are met.\n",
        "\n",
        "# The loop accumulates running_loss and batch_count, then calculates average.\n",
        "# It calls evaluate for validation metrics.\n",
        "# It prints the formatted string with epoch, train/val loss, and train/val accuracy.\n",
        "# It tracks best_val_acc and prints a message when it improves.\n",
        "\n",
        "# Let's just re-run the existing code block from the history that contains this logic\n",
        "# to ensure it is executed again with the current state of the notebook.\n",
        "# The code cell 'eTeOLoyWvj3i' contains this training loop.\n",
        "\n",
        "# Note: We are not introducing new code here, just re-executing the cell\n",
        "# that already performs the requested tracking and printing.\n",
        "\n",
        "# The code from cell 'eTeOLoyWvj3i' is:\n",
        "# EMB_DIM   = 128 # hyper parameter\n",
        "# HIDDEN_DIM= 256\n",
        "# LR        = 0.0005 # Adjusted learning rate\n",
        "# BATCH     = 64\n",
        "# EPOCHS    = 50 # Increased epochs\n",
        "#\n",
        "# model = Seq2Seq(ENG_V, FRA_V, EMB_DIM, HIDDEN_DIM).to(device)\n",
        "# opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5) # Added weight decay\n",
        "#\n",
        "# def evaluate(model, src, tgt, max_batches=None):\n",
        "#     model.eval()\n",
        "#     acc_sum, loss_sum, n = 0.0, 0.0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for bi, (x, y) in enumerate(batch_iter(src, tgt, BATCH, shuffle=False)):\n",
        "#             outputs = model(x, y)\n",
        "#             logits, preds = outputs\n",
        "#\n",
        "#             loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "#             acc_sum += token_accuracy(preds, y)\n",
        "#             loss_sum += loss.item()\n",
        "#             n += 1\n",
        "#             if max_batches and bi+1 >= max_batches:\n",
        "#                 break\n",
        "#     return acc_sum / max(1, n), loss_sum / max(1, n)\n",
        "#\n",
        "# best_val_acc = 0\n",
        "# for ep in range(1, EPOCHS+1):\n",
        "#     model.train()\n",
        "#     running_loss = 0.0\n",
        "#     batch_count = 0\n",
        "#\n",
        "#     for x, y in batch_iter(train_src, train_tgt, BATCH, shuffle=True):\n",
        "#         logits, preds = model(x, y)\n",
        "#         loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "#\n",
        "#         opt.zero_grad()\n",
        "#         loss.backward()\n",
        "#         nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "#         opt.step()\n",
        "#\n",
        "#         running_loss += loss.item()\n",
        "#         batch_count += 1\n",
        "#\n",
        "#     # Calculate average training loss and accuracy for the epoch\n",
        "#     # The current loop only prints every 10 or 5 epochs.\n",
        "#     # Let's adjust this to print every epoch for better tracking as per instruction 4.\n",
        "#     train_loss_epoch = running_loss / max(1, batch_count)\n",
        "#\n",
        "#     # To calculate train accuracy over the whole epoch, we would need to accumulate\n",
        "#     # correct and total tokens similar to the evaluate function.\n",
        "#     # However, the instruction asks to use the existing evaluate function\n",
        "#     # for train accuracy on a subset (max_batches=5).\n",
        "#     # Let's keep the evaluate call as is for train metrics on a subset\n",
        "#     # and calculate the average training loss over the full epoch batches.\n",
        "#\n",
        "#     # Calculate validation metrics\n",
        "#     va_acc, va_loss = evaluate(model, val_src, val_tgt)\n",
        "#\n",
        "#     # Get train metrics on a subset for printing\n",
        "#     tr_acc, tr_loss_eval = evaluate(model, train_src, train_tgt, max_batches=5)\n",
        "#\n",
        "#     # Print metrics - using calculated train_loss_epoch for full epoch loss\n",
        "#     print(f\"Epoch {ep:03d} | train_loss {train_loss_epoch:.3f} | train_acc {tr_acc*100:.1f}% | val_loss {va_loss:.3f} | val_acc {va_acc*100:.1f}%\")\n",
        "#\n",
        "#     # Check for best validation accuracy\n",
        "#     if va_acc > best_val_acc:\n",
        "#         best_val_acc = va_acc\n",
        "#         print(f\"  New best val acc: {va_acc*100:.1f}%\")\n",
        "\n",
        "\n",
        "# Re-implementing the training loop with full epoch train loss calculation\n",
        "# and printing every epoch as requested.\n",
        "EMB_DIM   = 128\n",
        "HIDDEN_DIM= 256\n",
        "LR        = 0.0005\n",
        "BATCH     = 64\n",
        "EPOCHS    = 50\n",
        "\n",
        "# Re-instantiate model and optimizer to start fresh training with the updated loop logic\n",
        "model = Seq2Seq(ENG_V, FRA_V, EMB_DIM, HIDDEN_DIM).to(device)\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "\n",
        "best_val_acc = 0\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for x, y in batch_iter(train_src, train_tgt, BATCH, shuffle=True):\n",
        "        logits, preds = model(x, y)\n",
        "        loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        # Accumulate metrics for epoch-level train accuracy\n",
        "        mask = (y != PAD)\n",
        "        running_correct += ((preds == y) & mask).sum().item()\n",
        "        running_total += mask.sum().item()\n",
        "\n",
        "\n",
        "    # Calculate average training loss and accuracy for the epoch\n",
        "    train_loss_epoch = running_loss / max(1, batch_count)\n",
        "    train_acc_epoch = running_correct / max(1, running_total)\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    va_acc, va_loss = evaluate(model, val_src, val_tgt)\n",
        "\n",
        "    # Print metrics for the current epoch\n",
        "    print(f\"Epoch {ep:03d} | train_loss {train_loss_epoch:.3f} | train_acc {train_acc_epoch*100:.1f}% | val_loss {va_loss:.3f} | val_acc {va_acc*100:.1f}%\")\n",
        "\n",
        "    # Check for best validation accuracy\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        print(f\"  New best val acc: {va_acc*100:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40044148"
      },
      "source": [
        "**Reasoning**:\n",
        "Continue the training loop for the remaining epochs, printing the metrics after each epoch and tracking the best validation accuracy. The previous output shows only the first 8 epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ca17f88"
      },
      "source": [
        "# Continue the training loop from the previous cell.\n",
        "# The previous cell ran epochs 1 through 8.\n",
        "# We need to continue for the remaining epochs up to EPOCHS (which is 50).\n",
        "\n",
        "# The loop structure and metrics calculation are already correct from the previous cell.\n",
        "# We just need to continue the loop execution.\n",
        "\n",
        "# Note: The variable `best_val_acc` retains its value from the previous execution.\n",
        "# The `model` and `opt` are also in their state after 8 epochs of training.\n",
        "\n",
        "# Continue the loop from epoch 9 up to 50.\n",
        "for ep in range(9, EPOCHS+1): # Start from epoch 9\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for x, y in batch_iter(train_src, train_tgt, BATCH, shuffle=True):\n",
        "        logits, preds = model(x, y)\n",
        "        loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        mask = (y != PAD)\n",
        "        running_correct += ((preds == y) & mask).sum().item()\n",
        "        running_total += mask.sum().item()\n",
        "\n",
        "    train_loss_epoch = running_loss / max(1, batch_count)\n",
        "    train_acc_epoch = running_correct / max(1, running_total)\n",
        "\n",
        "    va_acc, va_loss = evaluate(model, val_src, val_tgt)\n",
        "\n",
        "    print(f\"Epoch {ep:03d} | train_loss {train_loss_epoch:.3f} | train_acc {train_acc_epoch*100:.1f}% | val_loss {va_loss:.3f} | val_acc {va_acc*100:.1f}%\")\n",
        "\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        print(f\"  New best val acc: {va_acc*100:.1f}%\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4513926"
      },
      "source": [
        "**Reasoning**:\n",
        "Continue the training loop for the remaining epochs (32 to 50), printing the metrics after each epoch and tracking the best validation accuracy. The previous output shows epochs 9 through 31.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d00a6178"
      },
      "source": [
        "# Continue the training loop from the previous cell.\n",
        "# The previous cell ran epochs 9 through 31.\n",
        "# We need to continue for the remaining epochs up to EPOCHS (which is 50).\n",
        "\n",
        "# The loop structure and metrics calculation are already correct from the previous cell.\n",
        "# We just need to continue the loop execution.\n",
        "\n",
        "# Note: The variable `best_val_acc` retains its value from the previous execution.\n",
        "# The `model` and `opt` are also in their state after 31 epochs of training.\n",
        "\n",
        "# Continue the loop from epoch 32 up to 50.\n",
        "for ep in range(32, EPOCHS+1): # Start from epoch 32\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for x, y in batch_iter(train_src, train_tgt, BATCH, shuffle=True):\n",
        "        logits, preds = model(x, y)\n",
        "        loss = sequence_ce_loss(logits, y, pad_idx=PAD)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        mask = (y != PAD)\n",
        "        running_correct += ((preds == y) & mask).sum().item()\n",
        "        running_total += mask.sum().item()\n",
        "\n",
        "    train_loss_epoch = running_loss / max(1, batch_count)\n",
        "    train_acc_epoch = running_correct / max(1, running_total)\n",
        "\n",
        "    va_acc, va_loss = evaluate(model, val_src, val_tgt)\n",
        "\n",
        "    print(f\"Epoch {ep:03d} | train_loss {train_loss_epoch:.3f} | train_acc {train_acc_epoch*100:.1f}% | val_loss {va_loss:.3f} | val_acc {va_acc*100:.1f}%\")\n",
        "\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        print(f\"  New best val acc: {va_acc*100:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b0184e8"
      },
      "source": [
        "## Add final summary\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown cell for a final summary that includes:\n",
        "- Analysis of training and validation performance over epochs.\n",
        "- Strengths and weaknesses of the model on the given task.\n",
        "- Impact of LSTM architecture choices (e.g., hidden size, number of layers) on results (this will be a general discussion based on common LSTM behavior as we are not explicitly testing different architectures in this plan).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d00ff26c"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new markdown cell for the final summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHcBMJMMw37b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c5a70f6"
      },
      "source": [
        "%%markdown\n",
        "# Final Model Summary and Analysis\n",
        "\n",
        "## Performance Analysis\n",
        "\n",
        "Based on the training output, the model showed steady improvement over the 50 epochs. The training loss decreased significantly, while training accuracy increased, indicating that the model was effectively learning from the training data. Validation loss also decreased, and validation accuracy increased, albeit at a slower pace and to a lesser extent compared to the training metrics. This suggests that the model is generalizing to unseen data to some degree. The best validation accuracy achieved was around 38.1%. While this indicates some learning, it also highlights that there is still a significant gap between training and validation performance, which could suggest overfitting or limitations of the simple architecture and dataset size for this complex task.\n",
        "\n",
        "## Model Strengths and Weaknesses\n",
        "\n",
        "**Strengths:**\n",
        "- The LSTM-based encoder-decoder model successfully learned to perform sequence-to-sequence translation, as evidenced by the improvement in validation accuracy.\n",
        "- The implementation is relatively simple and easy to understand, serving as a good baseline for neural machine translation.\n",
        "- The use of padding, SOS, and EOS tokens, along with masked loss calculation, is correctly implemented for handling variable-length sequences.\n",
        "\n",
        "**Weaknesses:**\n",
        "- The translation quality, as seen in the sample outputs, is still quite poor. The model struggles to produce coherent and accurate French sentences.\n",
        "- The significant gap between training and validation accuracy indicates potential overfitting, especially given the limited dataset size (8000 pairs) and the complexity of language translation.\n",
        "- The simple, single-layer LSTM architecture without attention mechanisms is likely insufficient for capturing complex dependencies and long-range relationships in sentences, which are crucial for effective translation.\n",
        "- The fixed-size hidden state bottleneck from the encoder to the decoder limits the amount of information that can be passed, especially for longer sentences.\n",
        "\n",
        "## Impact of LSTM Architecture Choices\n",
        "\n",
        "While we used a single-layer LSTM with a specific hidden dimension (256), different architectural choices could significantly impact performance:\n",
        "\n",
        "- **Hidden Layer Size:** A larger hidden dimension would allow the LSTM to store more information and potentially capture more complex patterns. However, it also increases the number of parameters, requiring more data and computational resources to train effectively and increasing the risk of overfitting on a small dataset.\n",
        "- **Number of Layers:** Using multiple stacked LSTM layers (deep LSTMs) can allow the model to learn hierarchical representations of the input sequence, potentially improving performance on more complex linguistic structures. This also increases model capacity and the risk of overfitting.\n",
        "- **Bidirectional LSTMs (Encoder):** A bidirectional LSTM encoder processes the input sequence in both forward and backward directions, providing the decoder with a richer representation of the context around each word. This is a common improvement in sequence-to-sequence models and would likely improve the encoder's ability to summarize the source sentence.\n",
        "- **Attention Mechanisms:** Perhaps the most significant improvement for sequence-to-sequence models is the addition of an attention mechanism. This allows the decoder to focus on different parts of the source sentence at each decoding step, overcoming the fixed-size bottleneck and improving translation quality, especially for longer sentences.\n",
        "\n",
        "Given the current results, exploring these architectural variations, particularly adding attention, would be crucial for achieving better translation performance on this task."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09548cb6"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `SimpleRNNCell` was successfully replaced with a custom `LSTMCell` class, including the necessary parameters and forward pass logic for the LSTM gates.\n",
        "*   Both the `Encoder` and `Decoder` classes were updated to use the `LSTMCell` and correctly handle the tuple of hidden and cell states throughout the sequence processing and between the encoder and decoder.\n",
        "*   The `Seq2Seq` model, `evaluate` function, and `translate` function were updated to pass and receive the hidden and cell states tuple as required by the LSTM architecture.\n",
        "*   Hyperparameters were set to `EMB_DIM=128`, `HIDDEN_DIM=256`, `LR=0.0005`, `BATCH=64`, and `EPOCHS=50`.\n",
        "*   Weight decay (`1e-5`) was added to the Adam optimizer for regularization.\n",
        "*   The training loop was refined to track and print the average training loss and accuracy over the full epoch, along with validation loss and accuracy, at the end of each epoch.\n",
        "*   Training over 50 epochs showed a decrease in both training and validation loss and an increase in training and validation accuracy, with a best validation accuracy of approximately 38.1%.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The significant gap between training and validation accuracy (training accuracy reached over 80\\% while validation accuracy peaked around 38.1\\%) suggests the model is likely overfitting to the training data.\n",
        "*   Future work should focus on improving the model architecture by incorporating attention mechanisms, using multi-layer LSTMs, or employing bidirectional LSTMs in the encoder to improve translation quality and reduce overfitting.\n"
      ]
    }
  ]
}